{"cells":[{"cell_type":"markdown","id":"fddb44f5","metadata":{"id":"fddb44f5"},"source":["# 코로나 2020~ 2022 확진자 동향"]},{"cell_type":"code","execution_count":null,"id":"30c7afc1","metadata":{"id":"30c7afc1"},"outputs":[],"source":["import pandas as pd\n","import json, os\n","years=['2020','2021','2022']\n","i=0\n","while i < 3:\n","\n","    with open('csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n","        json_data = json.load(json_file)\n","    def country_name_convert(row):\n","        if row['Country_Region'] in json_data:\n","            return json_data[row['Country_Region']]\n","        return row['Country_Region']\n","    def create_dateframe(filename):\n","        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n","        try:\n","            doc = doc[['Country_Region', 'Confirmed']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n","        except:\n","            doc = doc[['Country/Region', 'Confirmed']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n","            doc.columns = ['Country_Region', 'Confirmed']\n","        doc = doc.dropna(subset=['Confirmed']) # 3. 특정 컬럼에 없는 데이터 삭제하기\n","        doc['Country_Region'] = doc.apply(country_name_convert, axis=1) # 4. 'Country_Region'의 국가명을 여러 파일에 일관되doc = doc.astype({'Confirmed': 'int64'}) # 5. 특정 컬럼의 데이터 타입 변경하기\n","        doc = doc.groupby('Country_Region').sum() # 6. 특정 컬럼으로 중복된 데이터를 합치기\n","        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n","        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n","        doc.columns = [date_column]\n","        return doc\n","\n","    def generate_dateframe_by_path(PATH):\n","\n","        #years=['2020','2021','2022']\n","\n","        file_list, csv_list = os.listdir(PATH), list()\n","        first_doc = True\n","        for file in file_list:\n","            if file.split(\"-\")[-1] == years[i]+'.csv':\n","                csv_list.append(file)\n","        csv_list.sort()\n","        for file in csv_list:\n","            doc = create_dateframe(file)\n","            if first_doc:\n","                final_doc, first_doc = doc, False\n","            else:\n","                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n","        final_doc = final_doc.fillna(0)\n","        return final_doc\n","    def create_flag_link(row):\n","        flag_link = 'https://flagcdn.com/48x36/' + row + '.png'\n","        return flag_link\n","\n","\n","\n","    PATH = 'csse_covid_19_data/csse_covid_19_daily_reports/'\n","    df_confirmed = generate_dateframe_by_path(PATH)\n","    df_confirmed = df_confirmed.astype('int64')\n","\n","    country_info = pd.read_csv(\"csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\", encoding='utf-8-sig',keep_default_na=False)\n","    country_info = country_info[['iso2', 'Country_Region']]\n","    country_info = country_info.drop_duplicates(subset='Country_Region', keep='first')\n","\n","    doc_final_country = pd.merge(df_confirmed, country_info, how='left', on='Country_Region')\n","    doc_final_country = doc_final_country.dropna(subset=['iso2'])\n","    doc_final_country['iso2'] = doc_final_country['iso2'].apply(create_flag_link)\n","\n","    cols = doc_final_country.columns.tolist()\n","    cols.remove('iso2')\n","    cols.insert(1, 'iso2')\n","    doc_final_country = doc_final_country[cols]\n","    cols[1] = 'Country_Flag'\n","    doc_final_country.columns = cols\n","    doc_final_country['Country_Flag'] = doc_final_country['Country_Flag'].str.lower()\n","\n","    doc_final_country.to_csv(\"def_covid_data_for_Recovered_\"+ years[i] + \".csv\")\n","    i+=1\n","\n","#years=['2020','2021','2022']\n","df_year=[]\n","for year in years:\n","    path= 'def_covid_data_for_Recovered_%s.csv' %year\n","    covid_df= pd.read_csv(path, encoding='utf-8-sig',index_col=0)\n","    df_year.append(covid_df)\n","\n","\n","\n","covid_df=pd.merge(df_year[0],df_year[1], on='Country_Region')\n","covid_df=pd.merge(covid_df,df_year[2], on='Country_Region')\n","cols_covic = covid_df.columns.tolist()\n","cols_covic.remove('Country_Flag_y')\n","cols_covic.remove('Country_Flag')\n","covid_df = covid_df[cols_covic]\n","cols_covic[1] = 'Country_Flag'\n","covid_df.columns = cols_covic\n","\n","\n","covid_df.to_csv(\"2020~2022.csv\")"]},{"cell_type":"markdown","id":"d50ae433","metadata":{"id":"d50ae433"},"source":["## 코로나 회복자 2020~ 2022"]},{"cell_type":"code","execution_count":null,"id":"4ada9e6e","metadata":{"collapsed":true,"id":"4ada9e6e","outputId":"2905fb21-3fb5-4318-cc13-7207f7d3cea5"},"outputs":[{"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)","Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flag_link\n\u001b[0;32m     52\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsse_covid_19_data/csse_covid_19_daily_reports/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 53\u001b[0m df_confirmed \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dateframe_by_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m df_confirmed \u001b[38;5;241m=\u001b[39m df_confirmed\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m country_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m,keep_default_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mgenerate_dateframe_by_path\u001b[1;34m(PATH)\u001b[0m\n\u001b[0;32m     37\u001b[0m csv_list\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_list:\n\u001b[1;32m---> 39\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dateframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_doc:\n\u001b[0;32m     41\u001b[0m         final_doc, first_doc \u001b[38;5;241m=\u001b[39m doc, \u001b[38;5;28;01mFalse\u001b[39;00m\n","Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mcreate_dateframe\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dateframe\u001b[39m(filename):\n\u001b[1;32m---> 14\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 1. csv 파일 읽기\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m         doc \u001b[38;5;241m=\u001b[39m doc[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry_Region\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecovered\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;66;03m# 2. 특정 컬럼만 선택해서 데이터프레임 만들기\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1235\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:544\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:633\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n","File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n","\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}],"source":["import pandas as pd\n","import json, os\n","years=['2020','2021','2022']\n","i=0\n","while i < 3:\n","\n","    with open('csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n","        json_data = json.load(json_file)\n","    def country_name_convert(row):\n","        if row['Country_Region'] in json_data:\n","            return json_data[row['Country_Region']]\n","        return row['Country_Region']\n","    def create_dateframe(filename):\n","        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n","        try:\n","            doc = doc[['Country_Region', 'Recovered']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n","        except:\n","            doc = doc[['Country/Region', 'Recovered']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n","            doc.columns = ['Country_Region', 'Recovered']\n","#오류로 인한 해당부분 삭제        #doc = doc.dropna(subset=['Confirmed']) # 3. 특정 컬럼에 없는 데이터 삭제하기\n","        doc['Country_Region'] = doc.apply(country_name_convert, axis=1) # 4. 'Country_Region'의 국가명을 여러 파일에 일관되doc = doc.astype({'Confirmed': 'int64'}) # 5. 특정 컬럼의 데이터 타입 변경하기\n","        doc = doc.groupby('Country_Region').sum() # 6. 특정 컬럼으로 중복된 데이터를 합치기\n","        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n","        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n","        doc.columns = [date_column]\n","        return doc\n","\n","    def generate_dateframe_by_path(PATH):\n","\n","        #years=['2020','2021','2022']\n","\n","        file_list, csv_list = os.listdir(PATH), list()\n","        first_doc = True\n","        for file in file_list:\n","            if file.split(\"-\")[-1] == years[i]+'.csv':\n","                csv_list.append(file)\n","        csv_list.sort()\n","        for file in csv_list:\n","            doc = create_dateframe(file)\n","            if first_doc:\n","                final_doc, first_doc = doc, False\n","            else:\n","                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n","        final_doc = final_doc.fillna(0)\n","        return final_doc\n","    def create_flag_link(row):\n","        flag_link = 'https://flagcdn.com/48x36/' + row + '.png'\n","        return flag_link\n","\n","\n","\n","    PATH = 'csse_covid_19_data/csse_covid_19_daily_reports/'\n","    df_confirmed = generate_dateframe_by_path(PATH)\n","    df_confirmed = df_confirmed.astype('int64')\n","\n","    country_info = pd.read_csv(\"csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\", encoding='utf-8-sig',keep_default_na=False)\n","    country_info = country_info[['iso2', 'Country_Region']]\n","    country_info = country_info.drop_duplicates(subset='Country_Region', keep='first')\n","\n","    doc_final_country = pd.merge(df_confirmed, country_info, how='left', on='Country_Region')\n","    doc_final_country = doc_final_country.dropna(subset=['iso2'])\n","    doc_final_country['iso2'] = doc_final_country['iso2'].apply(create_flag_link)\n","\n","    cols = doc_final_country.columns.tolist()\n","    cols.remove('iso2')\n","    cols.insert(1, 'iso2')\n","    doc_final_country = doc_final_country[cols]\n","    cols[1] = 'Country_Flag'\n","    doc_final_country.columns = cols\n","    doc_final_country['Country_Flag'] = doc_final_country['Country_Flag'].str.lower()\n","\n","    doc_final_country.to_csv(\"def_covid_data_for_Deaths_\"+ years[i] + \".csv\")\n","    i+=1\n","\n","#years=['2020','2021','2022']\n","df_year=[]\n","for year in years:\n","    path= 'def_covid_data_for_Recovered_%s.csv' %year\n","    covid_df= pd.read_csv(path, encoding='utf-8-sig',index_col=0)\n","    df_year.append(covid_df)\n","\n","\n","\n","covid_df=pd.merge(df_year[0],df_year[1], on='Country_Region')\n","covid_df=pd.merge(covid_df,df_year[2], on='Country_Region')\n","cols_covic = covid_df.columns.tolist()\n","cols_covic.remove('Country_Flag_y')\n","cols_covic.remove('Country_Flag')\n","covid_df = covid_df[cols_covic]\n","cols_covic[1] = 'Country_Flag'\n","covid_df.columns = cols_covic\n","\n","\n","covid_df.to_csv(\"Recovered_2020~2022.csv\")"]},{"cell_type":"markdown","id":"7c93f807","metadata":{"id":"7c93f807"},"source":["# 코로나 사망자 2020~ 2022"]},{"cell_type":"code","execution_count":null,"id":"14f297f9","metadata":{"id":"14f297f9"},"outputs":[],"source":["import pandas as pd\n","import json, os\n","years=['2020','2021','2022']\n","i=0\n","while i < 3:\n","\n","    with open('csse_covid_19_data/country_convert.json', 'r', encoding='utf-8-sig') as json_file:\n","        json_data = json.load(json_file)\n","    def country_name_convert(row):\n","        if row['Country_Region'] in json_data:\n","            return json_data[row['Country_Region']]\n","        return row['Country_Region']\n","    def create_dateframe(filename):\n","        doc = pd.read_csv(PATH + filename, encoding='utf-8-sig') # 1. csv 파일 읽기\n","        try:\n","            doc = doc[['Country_Region', 'Deaths']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n","        except:\n","            doc = doc[['Country/Region', 'Deaths']] # 2. 특정 컬럼만 선택해서 데이터프레임 만들기\n","            doc.columns = ['Country_Region', 'Deaths']\n","        doc = doc.dropna(subset=['Deaths']) # 3. 특정 컬럼에 없는 데이터 삭제하기\n","        doc['Country_Region'] = doc.apply(country_name_convert, axis=1) # 4. 'Country_Region'의 국가명을 여러 파일에 일관되doc = doc.astype({'Confirmed': 'int64'}) # 5. 특정 컬럼의 데이터 타입 변경하기\n","        doc = doc.groupby('Country_Region').sum() # 6. 특정 컬럼으로 중복된 데이터를 합치기\n","        # 7. 파일명을 기반으로 날짜 문자열 변환하고, 'Confirmed' 컬럼명 변경하기\n","        date_column = filename.split(\".\")[0].lstrip('0').replace('-', '/')\n","        doc.columns = [date_column]\n","        return doc\n","\n","    def generate_dateframe_by_path(PATH):\n","\n","        #years=['2020','2021','2022']\n","\n","        file_list, csv_list = os.listdir(PATH), list()\n","        first_doc = True\n","        for file in file_list:\n","            if file.split(\"-\")[-1] == years[i]+'.csv':\n","                csv_list.append(file)\n","        csv_list.sort()\n","        for file in csv_list:\n","            doc = create_dateframe(file)\n","            if first_doc:\n","                final_doc, first_doc = doc, False\n","            else:\n","                final_doc = pd.merge(final_doc, doc, how='outer', left_index=True, right_index=True)\n","        final_doc = final_doc.fillna(0)\n","        return final_doc\n","    def create_flag_link(row):\n","        flag_link = 'https://flagcdn.com/48x36/' + row + '.png'\n","        return flag_link\n","\n","\n","\n","    PATH = 'csse_covid_19_data/csse_covid_19_daily_reports/'\n","    df_confirmed = generate_dateframe_by_path(PATH)\n","    df_confirmed = df_confirmed.astype('int64')\n","\n","    country_info = pd.read_csv(\"csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv\", encoding='utf-8-sig',keep_default_na=False)\n","    country_info = country_info[['iso2', 'Country_Region']]\n","    country_info = country_info.drop_duplicates(subset='Country_Region', keep='first')\n","\n","    doc_final_country = pd.merge(df_confirmed, country_info, how='left', on='Country_Region')\n","    doc_final_country = doc_final_country.dropna(subset=['iso2'])\n","    doc_final_country['iso2'] = doc_final_country['iso2'].apply(create_flag_link)\n","\n","    cols = doc_final_country.columns.tolist()\n","    cols.remove('iso2')\n","    cols.insert(1, 'iso2')\n","    doc_final_country = doc_final_country[cols]\n","    cols[1] = 'Country_Flag'\n","    doc_final_country.columns = cols\n","    doc_final_country['Country_Flag'] = doc_final_country['Country_Flag'].str.lower()\n","\n","    doc_final_country.to_csv(\"def_covid_data_for_Deaths_\"+ years[i] + \".csv\")\n","    i+=1\n","\n","#years=['2020','2021','2022']\n","df_year=[]\n","for year in years:\n","    path= 'def_covid_data_for_Deaths_%s.csv' %year\n","    covid_df= pd.read_csv(path, encoding='utf-8-sig',index_col=0)\n","    df_year.append(covid_df)\n","\n","\n","\n","covid_df=pd.merge(df_year[0],df_year[1], on='Country_Region')\n","covid_df=pd.merge(covid_df,df_year[2], on='Country_Region')\n","cols_covic = covid_df.columns.tolist()\n","cols_covic.remove('Country_Flag_y')\n","cols_covic.remove('Country_Flag')\n","covid_df = covid_df[cols_covic]\n","cols_covic[1] = 'Country_Flag'\n","covid_df.columns = cols_covic\n","\n","\n","covid_df.to_csv(\"Deaths_2020~2022.csv\")"]},{"cell_type":"code","execution_count":null,"id":"6b2fe0bc","metadata":{"id":"6b2fe0bc"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}